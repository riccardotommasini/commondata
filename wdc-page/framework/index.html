<!DOCTYPE html>
<html><head><title>WDC - Extraction Framework</title>
<link rel="stylesheet" href="../style.css" type="text/css" media="screen"/>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="author" content="Robert Meusel, Hannes Mühleisen, Christian Bizer, Oliver Lehmberg">
<meta name="keywords" content="Distributed Framework, Java, Scalable, Cloud">
<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script type="text/javascript" src="/jquery.toc.min.js"></script>

<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-30248817-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
<style>
td.java, td.java-ln {vertical-align:top;}
tt.java, tt.java-ln, pre.java, pre.java-ln {line-height:1em; margin-bottom:0em;}
td.java-ln { text-align:right; }
tt.java-ln, pre.java-ln { color:#888888 }
/* Background       */ span.java0  { font-size: 10pt; color:#ffffff; }
/* Line numbers       */ span.java1  { font-size: 10pt; color:#808080; }
/* Multi-line comments       */ span.java2  { font-size: 10pt; color:#3f7f5f; }
/* Single-line comments       */ span.java3  { font-size: 10pt; color:#3f7f5f; }
/* Keywords       */ span.java4  { font-size: 10pt; color:#7f0055; font-weight:bold; }
/* Strings       */ span.java5  { font-size: 10pt; color:#2a00ff; }
/* Character constants       */ span.java6  { font-size: 10pt; color:#990000; }
/* Numeric constants       */ span.java7  { font-size: 10pt; color:#990000; }
/* Parenthesis       */ span.java8  { font-size: 10pt; color:#000000; }
/* Primitive Types       */ span.java9  { font-size: 10pt; color:#7f0055; font-weight:bold; }
/* Others       */ span.java10  { font-size: 10pt; color:#000000; }
/* Javadoc keywords       */ span.java11  { font-size: 10pt; color:#7f9fbf; }
/* Javadoc HTML tags       */ span.java12  { font-size: 10pt; color:#7f7f9f; }
/* Javadoc links       */ span.java13  { font-size: 10pt; color:#3f3fbf; }
/* Javadoc others       */ span.java14  { font-size: 10pt; color:#3f5fbf; }
/* Undefined       */ span.java15  { font-size: 10pt; color:#ff6100; }
/* Annotation       */ span.java16  { font-size: 10pt; color:#646464; }
</style>
</head>
<body> 

  <div id="logo" style="text-align:right; background-color: white;">&nbsp;&nbsp;<a href="http://dws.informatik.uni-mannheim.de"><img src="../images/ma-logo.gif" alt="University of Mannheim - Logo"></a>&nbsp;&nbsp;<br></div>


<div id="header">
<h1 style="font-size: 250%;">Web Data Commons - Extraction Framework</h1>
</div>

<div id="tagline">A Framework for the Distributed Processing of Large Web Crawls</div>

<div id="authors">
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/robert-meusel/">Robert Meusel</a><br />
<a href="http://hannes.muehleisen.org/">Hannes Mühleisen</a><br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/researchers/oliver-lehmberg/">Oliver Lehmberg</a><br />
Petar Petrovski<br />
<a href="http://dws.informatik.uni-mannheim.de/en/people/professors/prof-dr-christian-bizer/">Christian Bizer</a><br />
<br/>
<br/>
</div>

<div id="content">

<p>
This page provides an overview of the extraction framework which is used by
the Web Data Commons project to extract <a href="../structureddata">Microdata, Microformats and RDFa</a>
data, <a href="../hyperlinkgraph/">Web graphs</a>, and <a href="../webtables">HTML tables</a> from the web crawls provided by the
<a target= "_blank" href="http://commoncrawl.org/">Common Crawl Foundation</a>. The framework provides an easy to use basis for the distributed
processing of large web crawls using Amazon EC2 cloud services. The
framework is published under the terms of the Apache license and can be
simply customized to perform different data extraction tasks. This
documentation describes how the extraction framework is setup and how you
can customize it by implementing your own extractors.

<h2>Contents</h2>
<div id="toc" class="toc"></div>
<div id="toccontent">

<h2 id="general">1. General Information</h2>
<p>
The framework was developed by the WDC project to process a large number of files from the 
Common Crawl in parallel using the cloud services of Amazon. The extraction framework was originally designed by Hannes Mühleisen for the
extraction of Microdata, Microformats and RDFa from the 2012 Common Crawl
corpus. Later, it was extended to be able to extract hyperlink graphs as
well as the web tables. The software is written in Java using Maven as the build tool and can be run on any operating system. The current version (described in the following) allows
straightforward customization for various purposes. 

<br>
The picture below explains the principle process flow of the framework, which is basically steered by one master node, which can be either a local server or machine, or a cloud instance itself. 
<br>
<img src="../images/framework.png" alt="WDC Framework Process Flow">
<br>
<ol>
<li>From the master node the AWS Simple Queue Service is filled with all files which should be processed. Those files represent the task for the later working instances and are basically file references.</li>
<li>From the master node a number of instances are launched within the EC2 environment of AWS. After the start up, each instances will automatically install the WDC framework and launch it.</li>
<li>Each instance, after starting the framework, will automatically request a task from the SQS and start processing the file.</li>
<li>The file will first be downloaded from S3 and will then be processed by the worker.</li>
<li>After finishing one file, the result will be stored in the users own S3 Bucket. And the worker will start again at step 3.</li>
<li>After the queue is empty, the master can start collecting the extracted data and statistics.</li>
</ol>
</p>

<h2 id="start">2. Getting Started</h2>
<p>
Running your own extraction using the WDC Extraction Framework is rather simple and you only need a few minutes to set yourself up. Please follow the steps below and make sure you have all the requirements in place.
</p>

<h3 id="req">2.1. Requirements</h2>
<p>
In order to run an extraction using the WDC Framework the following is required:
<ul>
<li><b>Amazon Web Service Account/User</b>: As the framework is currently tailored to be run using services from Amazon, you will need to have an AWS with sufficient amount of credits. The user needs rights to access the AWS Services: EC2, S3, SQS and Simple DB.</li>
<li><b>AWS Access Token</b>: In order to make use of the Amazon services through the framework you need have your AWS <code>Access Key Id</code> as well as your <code>Secret Access Key</code>. Both is available through the Web Interface of AWS in the section <a href="http://aws.amazon.com/iam/">IAM</a> (See <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/IAM_Introduction.html">AWS IAM Docs</a>). In case you do not have access to this section with your user, contact the administrator of your AWS account.</li>
<li><b>Java</b>: One machine has to serve as master node. This machine, no matter if it is your local server/computer or an EC2 instance within AWS, needs to have Java 6 or higher (latest version runs on Java 8) installed to run the commands and control the extraction, e.g.: <pre><code>add-apt-repository -y ppa:webupd8team/java<br>apt-get update<br>sudo apt-get install oracle-java8-installer</code></pre></li>
<li><b>Maven</b>: In order to compile your own version of the framework you need to have <a href="http://maven.apache.org/">Maven</a>3 in place. Building the project with Maven2 will not work, as the project makes use of certain plugins, which are unknown to version 2 of Maven: <pre><code>apt-get install maven</code></pre></li>
<li><b>Subversion</b>: In order to get the code, you need subversion installed: <pre><code>apt-get install subversion</code></pre></li>
</ul>
</p>

<h3 id="build">2.2. Building the code</h3>
<p>
Below you find a very detailed step by step description how to build the WDC Extraction Framework:
<ol>
<li>Create a new folder for the repository and navigate to the folder: <pre><code>mkdir ~/framework<br>cd ~/framework</code></pre></li>
<li>Download the code of the framework from the <a href="https://www.assembla.com/">Assembla.com</a> <a href="https://www.assembla.com/code/commondata/subversion/nodes">WDC Repository</a>. The WDC Extraction Framework code is located under <code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/">WDCFramework/trunk/</a></code>: <br><pre><code>svn checkout https://subversion.assembla.com/svn/commondata/WDCFramework/trunk</code></pre></li>
<li>Create a copy of the <code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/src/main/resources/dpef.properties.dist">dpef.properties.dist</a></code> file within the <code>/src/main/resources</code> directory and name it <code>dpef.properties</code>. Within this file, all needed properties/configurations are stored: <pre><code>cp extractor/src/main/resource/dpef.properties.dist extractor/src/main/resource/dpef.properties</code></pre>
</li>
<li>Go through the file carefully and at least adjust all properties marked with <code>TODO</code>. Each property is described in more detail within the file.
The most important properties are listed below:
<ul>
<li><code>awsAccessKey</code> and <code>awsSecretKey</code>: Those keys are mandatory to get access to the AWS API and run any commands.</li>
<li><code>resultBucket</code> and <code>deployBucket</code>: Those S3 buckets will be used in the framework to store the results of the extraction and to store the packages .jar file for later deployment on any launched instance. You can create the buckets either via the Web Interface or using the command line tool <code>s3cmd</code>:
<pre><code>apt-get install s3cmd<br>s3cmd --configure<br>s3cmd mb s3://[resultbucketname]<br>s3cmd mb s3://[deploybucketname]</code></pre></li>
<li><code>ec2instancetype</code>: Defines the type of EC2 instances which will be launched. In order to find out which instance type suits your needs best, visit the <a href="https://aws.amazon.com/ec2/instance-types/">ec2 instance type website</a>. Former extractions by the WebDataCommons team were mainly done using <code>c1.xlarge</code> instances.
<br> Please note that the files of the current version of Common Crawl  can be found in the S3 bucket <code>commoncrawl</code> while the Common Crawl data bucket is <code>crawl-data</code>. Thus the property configuration should be as follows:
<pre><code>dataBucket = commoncrawl<br>dataPrefix = crawl-data<br>--bucket-prefix CC-MAIN-[...]/segments/[...]/warc/ (for October 2016: CC-MAIN-2016-44/segments/1476988717783.68/warc/)</code></pre> </li>
<li><code>processorClass</code>: This property defines which class is used for the extraction. So far three classes are implemented, which were used to run the former extractions of the WebDataCommons team:
<ul>
<li><code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/src/main/java/org/webdatacommons/hyperlinkgraph/processor/WatProcessor.java">org.webdatacommons.hyperlinkgraph.processor.WatProcessor</a></code>, which was used to extract the <a href="../hyperlinkgraph/index.html#toc3">hyperlink graph from the .wat files of the Spring 2014 Common Crawl dataset</a>.</li>
<li><code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/src/main/java/org/webdatacommons/structureddata/processor/ArcProcessor.java">org.webdatacommons.structureddata.processor.ArcProcessor</a></code>, which was used to extract the <a href="../structureddata/index.html#toc4">RDFa, Microdata and Microformats from the .arc files of 2012 Common Crawl dataset</a>.</li>
<li><code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/src/main/java/org/webdatacommons/structureddata/processor/WarcProcessor.java">org.webdatacommons.structureddata.processor.WarcProcessor</a></code>, which was used to extract the <a href="../structureddata/index.html">RDFa, Microdata and Microformats from the .warc files of the November 2013/2014/2015 Common Crawl dataset</a>.</li>
<li><code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/src/main/java/org/webdatacommons/webtables/processor/WarcProcessor.java">org.webdatacommons.webtables.processor.WarcProcessor</a></code>, which was used to extract the <a href="../webtables/index.html#toc2">Web Tables from the .warc files of the July 2015 Common Crawl dataset</a>. This code was mainly derived from the 
<a href="https://wwwdb.inf.tu-dresden.de/misc/dwtc/">Dresden Web Tables Corpus</a> repositories (<a href="https://github.com/JulianEberius/dwtc-extractor">Extractor</a>|<a href="https://github.com/JulianEberius/dwtc-tools">Tools</a>).</li>
</ul>
All these classes implement the <code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/src/main/java/org/webdatacommons/framework/processor/FileProcessor.java">org.webdatacommons.framework.processor.FileProcessor</a></code> interface.
</li>
</ul></li>
<li>Package the WDC Extraction Framework using Maven. Make sure you are using Maven3, as earlier versions will not work. <pre><code>mvn package</code></pre><b>Note</b>: When packaging the project for the first time, a large number of libraries will be downloaded into your .m2 directory, mainly from breda.informatik.uni-mannheim.de, which might take some time.<br>
After successfully packaging the project, there should be a new directory, named <code>target</code> within your root directory of the project. Besides others, this directory should include the packaged .jar file: <code>dpef-*.jar</code></li>
</ol>
</p>
<h3 id="runfirst">2.3 Running your first extraction</h3>
<p>
After you have packaged the code you can use the <code>bin/master</code> bash script to start and steer your extraction. This bash scripts calls functions implemented within the <code><a href="https://subversion.assembla.com/svn/commondata/WDCFramework/trunk/src/main/java/org/webdatacommons/framework/cli/Master.java">org.webdatacommons.framework.cli.Master</a></code> class. To execute the script you need to make it executable:<pre><code>chmod +x bin/master</code></pre>
By following the commands below, you will first push your .jar file to S3. Next, you fill the queue with tasks/files which need to be processed and then start a number of EC2 instances which will execute the files. You can monitor the extraction process and after finishing stop all instances again. It also includes commands to collect your results and store it to your local hard drive. <b>Note</b>: Please always keep in mind that the framework will make use of AWS services and that Amazon will charge you for their usage.
<ol>
<li><b>Deploy</b> to upload the JAR to S3:<pre><code>./bin/master deploy --jarfile target/dpef-*.jar</code></pre></li>
<li><b>Queue</b> to fill the extraction queue with the Common Crawl files you want to process: <pre><code>/bin/master queue --bucket-prefix CC-MAIN-2013-48/segments/1386163041297/wet/</code></pre> Please note, that the queue command is just fetching files within one folder. In case you need files located in different folders use the bucket prefix file option of the command. You can also limit the number of files pushed to the queue.</li>
<li><b>Start</b> to launch EC2 extraction instances from the spot market. The command will keep starting instances until it is cancelled, so beware! Also, the price limit has to be given. The current spot prices can be found at http://aws.amazon.com/ec2/spot-instances/#6. A general recommendation is to set this price at about the on-demand instance price. This way, we will benefit from the generally low spot prices without our extraction process being permanently killed. The price limit is given in US$.<pre><code>./bin/master start --worker-amount 10 --pricelimit 0.6</code></pre><b>Note</b>: It may take a while (approx. 10 Minutes) for the instances to become available and start taking tasks from the queue. You can observe the process of the spot requests within the AWS Web Dashboard.</li>
<li><b>Monitor</b> to monitor the process including the number of items in the queue, the approximate time to finish, and the number of running/requested instances<pre><code>./bin/master monitor</code></pre></li>
<li><b>Shutdown</b> to kill all worker nodes and terminate the spot instance requests<pre><code>./bin/master shutdown</code></pre></li>
<li><b>Retrieve Data</b> to retrieve all collected data to a local directory<pre><code>./bin/master  retrievedata --destination /some/directory</code></pre></li>
<li><b>Retrieve Stats</b> to retrieve all collected data statistics to a local directory from the Simple DB of AWS<pre><code>./bin/master  retrievestats --destination /some/directory</code></pre></li>
<li><b>Clear Queue</b> to remove all remaining tasks from the queue and delete them<pre><code>./bin/master clearqueue</code></pre></li>
<li><b>Clear Data</b> to remove all collected data from the Simple DB<pre><code>./bin/master cleardata</code></pre></li>
</ol>
For additional information and parameters which might be useful for your task you can have a look in the documentation of the different commands or have a look in the implementation class <code>org.webdatacommons.framework.cli.Master</a></code>.
</p>

<h2 id="customize">3. Customize your extraction</h2>
<p>
In this section we show how to build and run your own extractor. For this task we think about a large number of text files (plain .txt files), where each line consists of a number of chars. We are interested in all lines which only consists of numbers (matching the regex pattern [0-9]+). We want to extract those "numeric" lines to a new file and in addition we want to have two basic statistics about each file: the total number of lines of the parsed file and the number of lines which match our regex within the parsed file.
</p>

<h3 id="processor_prelim">3.1. Basics</h3>
<p>
Building your own extractor is easy. The core of each extractor is a so called processor within the WDC Extraction Framework. The processor in general gets an input file from the queue and processes it, stores statistics and handles the output. The framework itself takes care of the parallelization and administrative tasks like orchestrating the tasks. Therefore each processor needs to implement he <code>FileProcessor.java</code> interface:
<pre>
<tt class="java"><span class="java4">package </span><span class="java10">org.webdatacommons.framework.processor;<br />
</span><span class="java4">import </span><span class="java10">java.nio.channels.ReadableByteChannel;
</span><span class="java4">import </span><span class="java10">java.util.Map;<br />
</span><span class="java4">public interface </span><span class="java10">FileProcessor </span><span class="java8">{<br />
&#xA0; </span><span class="java10">Map&lt;String, String&gt; process</span><span class="java8">(</span><span class="java10">ReadableByteChannel fileChannel,
&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; String inputFileKey</span><span class="java8">) </span><span class="java4">throws </span><span class="java10">Exception;<br />
</span><span class="java8">}</span></tt>
</pre>
The interface is fairly simple, as it only contains one method, which receives a <code>ReadableByteChannel</code>, representing the file to process and the files name as input. As result the method returns a <code>Map&lt;String, String&gt;</code> of key value pairs containing statistics of the processed file. The returned key value pairs are written to the AWS Simple DB (see property <code>sdbdatadomain</code> of the <code>dpef.properties</code>). In case you do not need any statistics you can also return an empty <code>Map</code>. 
<br/><br/>
In addition, the WDC Extraction Framework offers a context class, named <code>ProcessingNode.java</code>, which allows the extending class to make use of contextual settings (e.g. AWS Services to store files). In many cases it makes sense to first have a look into this class before thinking about your own solution.
</p>
<h3 id="processor_custom">3.2. Building your own extractor</h3>
<p>
We create the <code>TextFileProcessor.java</code> which implements the necessary interface and extends the <code>ProcessingNode.java</code> to make use of the context (see line 14).
The class will process a text file, read each line, and check if the line consists only of digits. Lines matching this requirement will be written into a new output file:<br/>

<pre>
<tt class="java">
1  <span class="java4">package </span><span class="java10">org.webdatacommons.example.processor;

2  </span><span class="java4">import </span><span class="java10">java.io.BufferedReader;
3  </span><span class="java4">import </span><span class="java10">java.io.BufferedWriter;
4  </span><span class="java4">import </span><span class="java10">java.io.File;
5  </span><span class="java4">import </span><span class="java10">java.io.FileWriter;
6  </span><span class="java4">import </span><span class="java10">java.io.InputStreamReader;
7  </span><span class="java4">import </span><span class="java10">java.nio.channels.Channels;
8  </span><span class="java4">import </span><span class="java10">java.nio.channels.ReadableByteChannel;
9  </span><span class="java4">import </span><span class="java10">java.util.HashMap;
10 </span><span class="java4">import </span><span class="java10">java.util.Map;

11 </span><span class="java4">import </span><span class="java10">org.jets3t.service.model.S3Object;

12 </span><span class="java4">import </span><span class="java10">org.webdatacommons.framework.processor.FileProcessor;
13 </span><span class="java4">import </span><span class="java10">org.webdatacommons.framework.processor.ProcessingNode;

14 </span><span class="java4">public class </span><span class="java10">TextFileProcessor </span><span class="java4">extends </span><span class="java10">ProcessingNode </span><span class="java4">implements </span><span class="java10">FileProcessor </span><span class="java8">{
<br />
15 &#xA0; </span><span class="java16">@Override
16 &#xA0; </span><span class="java4">public </span><span class="java10">Map&lt;String, String&gt; process</span><span class="java8">(</span><span class="java10">ReadableByteChannel fileChannel,
17 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; String inputFileKey</span><span class="java8">) </span><span class="java4">throws </span><span class="java10">Exception </span><span class="java8">{

18 &#xA0;&#xA0;&#xA0; </span> <span class="java3">// initialize line count
19 &#xA0;&#xA0;&#xA0; </span><span class="java9">long </span><span class="java10">lnCnt = </span><span class="java7">0</span><span class="java10">;<br />
20 &#xA0;&#xA0;&#xA0; </span><span class="java3">// initialize match count
21 &#xA0;&#xA0;&#xA0; </span><span class="java9">long </span><span class="java10">mCnt = </span><span class="java7">0</span><span class="java10">;<br />
22 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Creating a buffered reader from the input stream - the channel is not compressed
23 &#xA0;&#xA0;&#xA0; </span><span class="java10">BufferedReader br = </span><span class="java4">new </span><span class="java10">BufferedReader</span><span class="java8">(</span><span class="java4">new </span><span class="java10">InputStreamReader</span><br/>&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; <span class="java8">(</span><span class="java10">Channels.newInputStream</span><span class="java8">(</span><span class="java10">fileChannel</span><span class="java8">)))</span><span class="java10">;
24 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Create a temporal file for our output
25 &#xA0;&#xA0;&#xA0; </span><span class="java10">File tempOutputFile = File.createTempFile</span><span class="java8">(
26 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java5">&#34;tmp_&#34; </span><span class="java10">+ inputFileKey.replace</span><span class="java8">(</span><span class="java5">&#34;/&#34;</span><span class="java10">, </span><span class="java5">&#34;_&#34;</span><span class="java8">)</span><span class="java10">, </span><span class="java5">&#34;.digitsonly.txt&#34;</span><span class="java8">)</span><span class="java10">;
27 &#xA0;&#xA0;&#xA0; </span><span class="java3">// we delete it on exit - so we do not flood the hard drive
28 &#xA0;&#xA0;&#xA0; </span><span class="java10">tempOutputFile.deleteOnExit</span><span class="java8">()</span><span class="java10">;
29 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Create the writer for the output
30 &#xA0;&#xA0;&#xA0; </span><span class="java10">BufferedWriter bw = </span><span class="java4">new </span><span class="java10">BufferedWriter</span><span class="java8">(</span><span class="java4">new </span><span class="java10">FileWriter</span><span class="java8">(</span><span class="java10">tempOutputFile</span><span class="java8">))</span><span class="java10">;

31 &#xA0;&#xA0;&#xA0; </span><span class="java4">while </span><span class="java8">(</span><span class="java10">br.ready</span><span class="java8">()){
32 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java3">// reading the channel line by line
33 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java10">String line = br.readLine</span><span class="java8">()</span><span class="java10">;
34 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; lnCnt++;
35 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java3">// Check if the line match our pattern
36 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java4">if </span><span class="java8">(</span><span class="java10">line != </span><span class="java4">null </span><span class="java10">&amp;&amp; line.matches</span><span class="java8">(</span><span class="java5">&#34;[0-9]+&#34;</span><span class="java8">)){
37 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java10">mCnt++;
38 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java3">// write the line to the output file
39 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java10">bw.write</span><span class="java8">(</span><span class="java10">line</span><span class="java8">)</span><span class="java10">;
40 &#xA0;&#xA0;&#xA0;&#xA0;&#xA0; </span><span class="java8">}
41 &#xA0;&#xA0;&#xA0; }
42 &#xA0;&#xA0;&#xA0; </span><span class="java10">br.close</span><span class="java8">()</span><span class="java10">;
43 &#xA0;&#xA0;&#xA0; bw.close</span><span class="java8">()</span><span class="java10">;<br />
44 &#xA0;&#xA0;&#xA0; </span><span class="java3">// Now the file is parsed completely and the output needs to be stored to s3
45 &#xA0;&#xA0;&#xA0; // create an s3 object
46 &#xA0;&#xA0;&#xA0; </span><span class="java10">S3Object dataFileObject = </span><span class="java4">new </span><span class="java10">S3Object</span><span class="java8">(</span><span class="java10">tempOutputFile</span><span class="java8">)</span><span class="java10">;
47 &#xA0;&#xA0;&#xA0; </span><span class="java3">// name the file
48 &#xA0;&#xA0;&#xA0; </span><span class="java10">String outputFileKey = inputFileKey.replace</span><span class="java8">(</span><span class="java5">&#34;/&#34;</span><span class="java10">, </span><span class="java5">&#34;_&#34;</span><span class="java8">) </span><span class="java10">+ </span><span class="java5">&#34;digitsonly.txt&#34;</span><span class="java10">;
49 &#xA0;&#xA0;&#xA0; </span><span class="java3">// set the name
50 &#xA0;&#xA0;&#xA0; </span><span class="java10">dataFileObject.setKey</span><span class="java8">(</span><span class="java10">outputFileKey</span><span class="java8">)</span><span class="java10">;
51 &#xA0;&#xA0;&#xA0; </span><span class="java3">// put the object to the result bucket
52 &#xA0;&#xA0;&#xA0; </span><span class="java10">getStorage</span><span class="java8">()</span><span class="java10">.putObject</span><span class="java8">(</span><span class="java10">getOrCry</span><span class="java8">(</span><span class="java5">&#34;resultBucket&#34;</span><span class="java8">)</span><span class="java10">, dataFileObject</span><span class="java8">)</span><span class="java10">;

53 &#xA0;&#xA0;&#xA0; </span><span class="java3">// create the statistic map (key, value) for this file
54 &#xA0;&#xA0;&#xA0; </span><span class="java10">Map&lt;String, String&gt; map = </span><span class="java4">new </span><span class="java10">HashMap&lt;String, String&gt;</span><span class="java8">()</span><span class="java10">;
55 &#xA0;&#xA0;&#xA0; map.put</span><span class="java8">(</span><span class="java5">&#34;lines_total&#34;</span><span class="java10">, String.valueOf</span><span class="java8">(</span><span class="java10">lnCnt</span><span class="java8">))</span><span class="java10">;
56 &#xA0;&#xA0;&#xA0; map.put</span><span class="java8">(</span><span class="java5">&#34;lines_match&#34;</span><span class="java10">, String.valueOf</span><span class="java8">(</span><span class="java10">mCnt</span><span class="java8">))</span><span class="java10">;
57 &#xA0;&#xA0;&#xA0; </span><span class="java4">return </span><span class="java10">map;
58 &#xA0; </span><span class="java8">}
59 }</span></tt>
</pre>
The code processes the file in the following way:
<dl>
<dt>Line 18 to 21</dt><dd>Initializing basic statistic counts.</dd>
<dt>Line 22 to 23</dt><dd>Creating a <code>BufferedReader</code> from the <code>ReadableByteChannel</code>, which makes it easy to read the input file line by line.</dd>
<dt>Line 24 to 30</dt><dd>Creating a <code>BufferedWriter</code> for a temporal file, which will be deleted as soon as we exit it. This ensures that the hard drive will not be flooded, when processing a large number of files or large files.</dd>
<dt>Line 31 to 43</dt><dd>Processing the current file line by line, counting each line. In case a line matches the regex ([0-9]+), the line is written - as it is - into the output file and the corresponding counter is increased.</dd>
<dt>Line 44 to 52</dt><dd>Writing the output file, which includes only the lines consisting of numbers to S3 using the context from the <code>ProcessingNode</code>.</dd>
<dt>Line 53 to 56</dt><dd>The two basic statistics, number of lines and number of matching lines are put into the map and returned by the function. The statistics are written into the AWS Simple DB automatically by the invoking method.</dd>
</dl>
</p>
<h3 id="package_custom">3.3. Configure and package your new extractor</h3>
<p>
As the extractor is implemented, you need to adjust the <code>dpef.properties</code> file before packaging your code again. First, the new processor needs to be included:
<pre><code>processorClass = org.webdatacommons.example.processor.TextFileProcessor</code></pre>
As we want to parse plain text files, the data suffix property needs to be adjusted to fit our input file suffixes, e.g.:
<pre><code>dataSuffix = .txt</code></pre>
Having adjust all necessary properties you can package the project and follow the commands as described <a href="#toc4">above</a>.
</p>
<h3 id="limits_custom">3.4. Limitations</h3>
<p>
Although the current version of the WDC Extraction Framework allows a straightforward customization for various kinds of extractions, there are some limitations/restrictions which are not (yet) addressed within this version:
<ul>
<li>In general, the framework is tailored to be used with AWS only as it depends on key services provided by AWS, the SQS and S3.</li>
<li>Input files, which should be processed need to be stored and available within AWS S3. The framework currently does not allow you to queue and process files stored.</li>
<li>Statistics, created while processing files will be stored with AWS SimpleDB. At the moment is not possible to switch this option on/off or change the location, e.g. to an own data base.</li>
</ul>
In case you customize the framework in any case, and think this could be helpful for others, we would be glad to hear about and integrate your improvements within the repository and make them public to everybody.
</p>
<h2 id="costs">4. Former extractors</h2>
<p>
Some of the datasets, which are available at the Web Data Commons website were extracted using a previous, not that framework-like version of the code. In case, you want to re-run some of the former extractions, or need some inspiration on how to processes certain types of files (e.g. .arc, .warc., ...), the code is still available within the WDC repository below <code><a href="https://subversion.assembla.com/svn/commondata/Extractor/trunk/extractor/">Extractor/trunk/extractor</a></code>.
</p>
<h2 id="costs">5. Costs</h2>
<p>
The usage of the WDC framework is free of charge. Nevertheless, as the framework makes use of services from AWS, Amazon will charge you for the usage.
There are several granting possibilities by Amazon itself, where Amazon supports ideas and projects running within their cloud system with free credits - especially in the education area (see <a href="http://aws.amazon.com/grants/">Amazon Grants</a>).
</p>

<h2 id="license">6. License</h2>

<p>The Web Data Commons extraction framework can be used under the terms of the <a href="http://www.apache.org/licenses/LICENSE-2.0">Apache Software License</a>. </p>

<h2 id="feedback">7. Feedback</h2>
<p>Please send questions and feedback to the <a href="http://groups.google.com/group/web-data-commons">Web Data Commons mailing list</a> or post them in our <a href="https://groups.google.com/forum/?fromgroups#!forum/web-data-commons">Web Data Commons Google Group</a>.<br/><br/>
More information about Web Data Commons is found <a href="../index.html">here</a>.</p>
</div>
</div>

<script type="text/javascript">
$('#toc').toc({
    'selectors': 'h2,h3', //elements to use as headings
    'container': '#toccontent', //element to find all selectors in
    'smoothScrolling': true, //enable or disable smooth scrolling on click
    'prefix': 'toc', //prefix for anchor tags and class names
    'highlightOnScroll': true, //add class to heading that is currently in focus
    'highlightOffset': 100, //offset to trigger the next headline
    'anchorName': function(i, heading, prefix) { //custom function for anchor name
        return prefix+i;
    } 
});
</script>

</body>
</html> 
